{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "device = \"cuda:4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import abc\n",
    "from typing import Union, Tuple\n",
    "from diffusers import DDIMScheduler\n",
    "from utils import load_image_data, load_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "Util functions based on Diffuser framework and Masactrl.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyControl:\n",
    "\n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "\n",
    "    def between_steps(self):\n",
    "        return\n",
    "\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        return attn\n",
    "\n",
    "\n",
    "class AttentionControl(abc.ABC):\n",
    "\n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "\n",
    "    def between_steps(self):\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return self.num_att_layers if self.LOW_RESOURCE else 0\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
    "            h = attn.shape[0]\n",
    "            attn[h // 2 :] = self.forward(attn[h // 2 :], is_cross, place_in_unet)\n",
    "        self.cur_att_layer += 1\n",
    "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step += 1\n",
    "            # self.between_steps()\n",
    "        return attn\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "        self.LOW_RESOURCE = False\n",
    "\n",
    "\n",
    "class AttentionStore(AttentionControl):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\n",
    "            \"down_cross\": [],\n",
    "            \"mid_cross\": [],\n",
    "            \"up_cross\": [],\n",
    "            \"down_self\": [],\n",
    "            \"mid_self\": [],\n",
    "            \"up_self\": [],\n",
    "        }\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "        # if attn.shape[1] <= 16 ** 2:  # avoid memory overhead\n",
    "        #     self.step_store[key].append(attn)\n",
    "        return attn\n",
    "\n",
    "    def between_steps(self):\n",
    "        if len(self.attention_store) == 0:\n",
    "            self.attention_store = self.step_store\n",
    "        else:\n",
    "            for key in self.attention_store:\n",
    "                for i in range(len(self.attention_store[key])):\n",
    "\n",
    "                    self.attention_store[key][i] += self.step_store[key][i]\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def get_average_attention(self):\n",
    "        average_attention = {\n",
    "            key: [item / self.cur_step for item in self.attention_store[key]]\n",
    "            for key in self.attention_store\n",
    "        }\n",
    "        return average_attention\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "\n",
    "class SelfAttentionControlEdit(AttentionStore, abc.ABC):\n",
    "\n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "\n",
    "    def replace_self_attention(self, attn_base, att_replace, place_in_unet):\n",
    "        if att_replace.shape[2] <= 32**2:\n",
    "            attn_base = attn_base.unsqueeze(0).expand(\n",
    "                att_replace.shape[0], *attn_base.shape\n",
    "            )\n",
    "            return attn_base\n",
    "        else:\n",
    "            return att_replace\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        super(SelfAttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
    "        if is_cross or (\n",
    "            self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]\n",
    "        ):\n",
    "            h = attn.shape[0] // (self.batch_size)\n",
    "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
    "            attn_base, attn_repalce = attn[0], attn[1:]\n",
    "            if is_cross:\n",
    "                pass\n",
    "            else:\n",
    "                attn[1:] = self.replace_self_attention(\n",
    "                    attn_base, attn_repalce, place_in_unet\n",
    "                )\n",
    "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
    "        return attn\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompts,\n",
    "        num_steps: int,\n",
    "        self_replace_steps: Union[float, Tuple[float, float]],\n",
    "    ):\n",
    "        super(SelfAttentionControlEdit, self).__init__()\n",
    "        self.batch_size = len(prompts)\n",
    "        if type(self_replace_steps) is float:\n",
    "            self_replace_steps = 0, self_replace_steps\n",
    "        self.num_self_replace = int(num_steps * self_replace_steps[0]), int(\n",
    "            num_steps * self_replace_steps[1]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_attention_control_new(model, controller):\n",
    "    def ca_forward(self, place_in_unet):\n",
    "        to_out = self.to_out\n",
    "        if type(to_out) is torch.nn.modules.container.ModuleList:\n",
    "            to_out = self.to_out[0]\n",
    "        else:\n",
    "            to_out = self.to_out\n",
    "\n",
    "        def forward(hidden_states, encoder_hidden_states=None, attention_mask=None, **cross_attention_kwargs):\n",
    "            x = hidden_states\n",
    "            context = encoder_hidden_states\n",
    "            mask = attention_mask\n",
    "            batch_size, sequence_length, dim = x.shape\n",
    "            h = self.heads\n",
    "            q = self.to_q(x)\n",
    "            is_cross = context is not None\n",
    "            context = context if is_cross else x\n",
    "            k = self.to_k(context)\n",
    "            v = self.to_v(context)\n",
    "            q = self.head_to_batch_dim(q)\n",
    "            k = self.head_to_batch_dim(k)\n",
    "            v = self.head_to_batch_dim(v)\n",
    "\n",
    "            sim = torch.einsum(\"b i d, b j d -> b i j\", q, k) * self.scale\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = mask.reshape(batch_size, -1)\n",
    "                max_neg_value = -torch.finfo(sim.dtype).max\n",
    "                mask = mask[:, None, :].repeat(h, 1, 1)\n",
    "                sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "            # attention, what we cannot get enough of\n",
    "            attn = sim.softmax(dim=-1)\n",
    "            # if is_cross:\n",
    "            #     ssss = []\n",
    "            attn = controller(attn, is_cross, place_in_unet)\n",
    "            out = torch.einsum(\"b i j, b j d -> b i d\", attn, v)\n",
    "            out = self.batch_to_head_dim(out)\n",
    "            return to_out(out)\n",
    "\n",
    "        return forward\n",
    "\n",
    "    class DummyController:\n",
    "\n",
    "        def __call__(self, *args):\n",
    "            return args[0]\n",
    "\n",
    "        def __init__(self):\n",
    "            self.num_att_layers = 0\n",
    "\n",
    "    if controller is None:\n",
    "        controller = DummyController()\n",
    "\n",
    "    def register_recr(net_, count, place_in_unet):\n",
    "        # print(net_.__class__.__name__)\n",
    "        if net_.__class__.__name__ == 'Attention':\n",
    "            net_.forward = ca_forward(net_, place_in_unet)\n",
    "            return count + 1\n",
    "        elif hasattr(net_, 'children'):\n",
    "            for net__ in net_.children():\n",
    "                count = register_recr(net__, count, place_in_unet)\n",
    "        return count\n",
    "\n",
    "    cross_att_count = 0\n",
    "    sub_nets = model.unet.named_children()\n",
    "    # print(sub_nets)\n",
    "    for net in sub_nets:\n",
    "        if \"down\" in net[0]:\n",
    "            cross_att_count += register_recr(net[1], 0, \"down\")\n",
    "        elif \"up\" in net[0]:\n",
    "            cross_att_count += register_recr(net[1], 0, \"up\")\n",
    "        elif \"mid\" in net[0]:\n",
    "            cross_att_count += register_recr(net[1], 0, \"mid\")\n",
    "\n",
    "    controller.num_att_layers = cross_att_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreePromptPipeline(StableDiffusionPipeline):\n",
    "\n",
    "    def next_step(\n",
    "        self,\n",
    "        model_output: torch.FloatTensor,\n",
    "        timestep: int,\n",
    "        x: torch.FloatTensor,\n",
    "        eta=0.,\n",
    "        verbose=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inverse sampling for DDIM Inversion\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"timestep: \", timestep)\n",
    "        next_step = timestep\n",
    "        timestep = min(timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps, 999)\n",
    "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep] if timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "        alpha_prod_t_next = self.scheduler.alphas_cumprod[next_step]\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        pred_x0 = (x - beta_prod_t**0.5 * model_output) / alpha_prod_t**0.5\n",
    "        pred_dir = (1 - alpha_prod_t_next)**0.5 * model_output\n",
    "        x_next = alpha_prod_t_next**0.5 * pred_x0 + pred_dir\n",
    "        return x_next, pred_x0\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        model_output: torch.FloatTensor,\n",
    "        timestep: int,\n",
    "        x: torch.FloatTensor,\n",
    "        eta: float=0.0,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        predict the sampe the next step in the denoise process.\n",
    "        \"\"\"\n",
    "        prev_timestep = timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps\n",
    "        alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
    "        alpha_prod_t_prev = self.scheduler.alphas_cumprod[prev_timestep] if prev_timestep > 0 else self.scheduler.final_alpha_cumprod\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        pred_x0 = (x - beta_prod_t**0.5 * model_output) / alpha_prod_t**0.5\n",
    "        pred_dir = (1 - alpha_prod_t_prev)**0.5 * model_output\n",
    "        x_prev = alpha_prod_t_prev**0.5 * pred_x0 + pred_dir\n",
    "        return x_prev, pred_x0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def image2latent(self, image):\n",
    "        # input image density range [-1, 1]\n",
    "        latents = self.vae.encode(image)['latent_dist'].mean\n",
    "        latents = latents * 0.18215\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def latent2image(self, latents, return_type='np'):\n",
    "        latents = 1 / 0.18215 * latents.detach()\n",
    "        image = self.vae.decode(latents)['sample']\n",
    "        if return_type == 'np':\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        elif return_type == \"pt\":\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "        return image\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt,\n",
    "        batch_size=1,\n",
    "        height=512,\n",
    "        width=512,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        eta=0.0,\n",
    "        latents=None,\n",
    "        unconditioning=None,\n",
    "        neg_prompt=None,\n",
    "        ref_intermediate_latents=None,\n",
    "        return_intermediates=False,\n",
    "        **kwds):\n",
    "        if isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        elif isinstance(prompt, str):\n",
    "            if batch_size > 1:\n",
    "                prompt = [prompt] * batch_size\n",
    "\n",
    "        text_input = self.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
    "        if kwds.get(\"dir\"):\n",
    "            dir = text_embeddings[-2] - text_embeddings[-1]\n",
    "            u, s, v = torch.pca_lowrank(dir.transpose(-1, -2), q=1, center=True)\n",
    "            text_embeddings[-1] = text_embeddings[-1] + kwds.get(\"dir\") * v\n",
    "            print(u.shape)\n",
    "            print(v.shape)\n",
    "\n",
    "        # define initial latents\n",
    "        latents_shape = (batch_size, self.unet.config.in_channels, height//8, width//8)\n",
    "        if latents is None:\n",
    "            latents = torch.randn(latents_shape, device=self.device)\n",
    "        else:\n",
    "            assert latents.shape == latents_shape, f\"The shape of input latent tensor {latents.shape} should equal to predefined one.\"\n",
    "\n",
    "        # unconditional embedding for classifier free guidance\n",
    "        if guidance_scale > 1.:\n",
    "            max_length = text_input.input_ids.shape[-1]\n",
    "            if neg_prompt:\n",
    "                uc_text = neg_prompt\n",
    "            else:\n",
    "                uc_text = \"\"\n",
    "            # uc_text = \"ugly, tiling, poorly drawn hands, poorly drawn feet, body out of frame, cut off, low contrast, underexposed, distorted face\"\n",
    "            unconditional_input = self.tokenizer(\n",
    "                [uc_text] * batch_size,\n",
    "                padding=\"max_length\",\n",
    "                max_length=77,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            # unconditional_input.input_ids = unconditional_input.input_ids[:, 1:]\n",
    "            unconditional_embeddings = self.text_encoder(unconditional_input.input_ids.to(self.device))[0]\n",
    "            text_embeddings = torch.cat([unconditional_embeddings, text_embeddings], dim=0) #两个空文本\n",
    "\n",
    "\n",
    "        # iterative sampling\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        latents_list = [latents]\n",
    "        pred_x0_list = [latents]\n",
    "        for i, t in enumerate(tqdm(self.scheduler.timesteps, desc=\"DDIM Sampler\")):\n",
    "            if ref_intermediate_latents is not None:\n",
    "                # note that the batch_size >= 2\n",
    "                latents_ref = ref_intermediate_latents[-1 - i]\n",
    "\n",
    "\n",
    "                _, latents_cur = latents.chunk(2)\n",
    "\n",
    "\n",
    "                latents = torch.cat([latents_ref, latents_cur])\n",
    "\n",
    "            if guidance_scale > 1.:\n",
    "                model_inputs = torch.cat([latents] * 2)\n",
    "            else:\n",
    "                model_inputs = latents\n",
    "            if unconditioning is not None and isinstance(unconditioning, list):\n",
    "                _, text_embeddings = text_embeddings.chunk(2)\n",
    "                text_embeddings = torch.cat([unconditioning[i].expand(*text_embeddings.shape), text_embeddings]) \n",
    "            # predict tghe noise\n",
    "\n",
    "\n",
    "            noise_pred = self.unet(model_inputs, t, encoder_hidden_states=text_embeddings).sample\n",
    "            if guidance_scale > 1.:\n",
    "                noise_pred_uncon, noise_pred_con = noise_pred.chunk(2, dim=0)\n",
    "                noise_pred = noise_pred_uncon + guidance_scale * (noise_pred_con - noise_pred_uncon)\n",
    "            # compute the previous noise sample x_t -> x_t-1\n",
    "            latents, pred_x0 = self.step(noise_pred, t, latents)\n",
    "            latents_list.append(latents)\n",
    "            pred_x0_list.append(pred_x0)\n",
    "\n",
    "        image = self.latent2image(latents, return_type=\"pt\")\n",
    "        if return_intermediates:\n",
    "            pred_x0_list = [self.latent2image(img, return_type=\"pt\") for img in pred_x0_list]\n",
    "            latents_list = [self.latent2image(img, return_type=\"pt\") for img in latents_list]\n",
    "            return image, pred_x0_list, latents_list\n",
    "        return image\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def invert(\n",
    "        self,\n",
    "        image: torch.Tensor,\n",
    "        prompt,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        eta=0.0,\n",
    "        return_intermediates=False,\n",
    "        **kwds):\n",
    "        \"\"\"\n",
    "        invert a real image into noise map with determinisc DDIM inversion\n",
    "        \"\"\"\n",
    "        batch_size = image.shape[0]\n",
    "        if isinstance(prompt, list):\n",
    "            if batch_size == 1:\n",
    "                image = image.expand(len(prompt), -1, -1, -1)\n",
    "        elif isinstance(prompt, str):\n",
    "            if batch_size > 1:\n",
    "                prompt = [prompt] * batch_size\n",
    "\n",
    "        # text embeddings\n",
    "        text_input = self.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
    "        # define initial latents\n",
    "        latents = self.image2latent(image)\n",
    "        start_latents = latents\n",
    "\n",
    "        # exit()\n",
    "        # unconditional embedding for classifier free guidance\n",
    "        if guidance_scale > 1.:\n",
    "            max_length = text_input.input_ids.shape[-1]\n",
    "            unconditional_input = self.tokenizer(\n",
    "                [\"\"] * batch_size,\n",
    "                padding=\"max_length\",\n",
    "                max_length=77,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            unconditional_embeddings = self.text_encoder(unconditional_input.input_ids.to(self.device))[0]\n",
    "            text_embeddings = torch.cat([unconditional_embeddings, text_embeddings], dim=0)\n",
    "\n",
    "        # interative sampling\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "        latents_list = [latents]\n",
    "        pred_x0_list = [latents]\n",
    "        for i, t in enumerate(tqdm(reversed(self.scheduler.timesteps), desc=\"DDIM Inversion\")):\n",
    "            if guidance_scale > 1.:\n",
    "                model_inputs = torch.cat([latents] * 2)\n",
    "            else:\n",
    "                model_inputs = latents\n",
    "\n",
    "            # predict the noise\n",
    "            noise_pred = self.unet(model_inputs, t, encoder_hidden_states=text_embeddings).sample\n",
    "            if guidance_scale > 1.:\n",
    "                noise_pred_uncon, noise_pred_con = noise_pred.chunk(2, dim=0)\n",
    "                noise_pred = noise_pred_uncon + guidance_scale * (noise_pred_con - noise_pred_uncon)\n",
    "            # compute the previous noise sample x_t-1 -> x_t\n",
    "            latents, pred_x0 = self.next_step(noise_pred, t, latents)\n",
    "            latents_list.append(latents)\n",
    "            pred_x0_list.append(pred_x0)\n",
    "\n",
    "        if return_intermediates:\n",
    "            # return the intermediate laters during inversion\n",
    "            # pred_x0_list = [self.latent2image(img, return_type=\"pt\") for img in pred_x0_list]\n",
    "            return latents, latents_list\n",
    "        return latents, start_latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = FreePromptPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", safety_checker=None\n",
    ").to(device)\n",
    "pipe.scheduler = DDIMScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = load_image_data(\"images/data.json\")\n",
    "print(len(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editing_data = image_data[4]\n",
    "path, source_caption, target_caption = (\n",
    "    editing_data[\"path\"],\n",
    "    editing_data[\"source_caption\"],\n",
    "    editing_data[\"target_caption\"],\n",
    ")\n",
    "print(source_caption)\n",
    "print(target_caption)\n",
    "images = load_image(path)\n",
    "images.resize((256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_caption = \"a cat\"\n",
    "target_caption = \"a egg\"\n",
    "source_image = torch.from_numpy(np.array(images)).float().permute(2, 0, 1) / 127.5 - 1\n",
    "source_image = source_image.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_replace_steps = 0.8\n",
    "NUM_DIFFUSION_STEPS = 50\n",
    "\n",
    "# invert the source image\n",
    "start_code, latents_list = pipe.invert(\n",
    "    source_image,\n",
    "    source_caption,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=50,\n",
    "    return_intermediates=True,\n",
    ")\n",
    "\n",
    "\n",
    "latents = torch.randn(start_code.shape, device=device)\n",
    "prompts = [source_caption, target_caption]\n",
    "\n",
    "start_code = start_code.expand(len(prompts), -1, -1, -1)\n",
    "controller = SelfAttentionControlEdit(\n",
    "    prompts, NUM_DIFFUSION_STEPS, self_replace_steps=self_replace_steps\n",
    ")\n",
    "\n",
    "register_attention_control_new(pipe, controller)\n",
    "\n",
    "# Note: querying the inversion intermediate features latents_list\n",
    "# may obtain better reconstruction and editing results\n",
    "results = pipe(\n",
    "    prompts,\n",
    "    latents=start_code,\n",
    "    guidance_scale=7.5,\n",
    "    ref_intermediate_latents=latents_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(images):\n",
    "    images = images.cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (images * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "    # Plot the images\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(images[i])\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ldm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "587aa04bacead72c1ffd459abbe4c8140b72ba2b534b24165b36a2ede3d95042"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
